Kubernetes Notes
1. Pods in Kubernetes

Pod: The smallest deployable unit in Kubernetes.
Defined using YAML manifest files.
You can check supported API versions with:
kubectl explain pod

2. YAML Syntax (Key Concepts)

a) spec.selector.matchLabels
Defines which Pods a Deployment should manage.
Works like a filter: â€œSelect Pods with these labels.â€
b) spec.template.metadata.labels
Labels that are actually applied to Pods when created.
Defines what labels each Pod will have.

3. Deployments & Replica Management

a) ReplicationController
Ensures a specified number of Pods are always running.
Uses:
Selector â†’ to match and manage Pods by labels.
template.metadata.labels â†’ applies labels to new Pods.

b) Deployment
Manages ReplicaSets (which in turn manage Pods).
Provides features like updates, rollbacks, and scaling.

4. Updating a Deployment

a) Rolling Updates (Default Strategy)
Ensures zero downtime updates.
Process:
Kubernetes creates new Pods with the updated spec (e.g., new image).
Waits until new Pods are Ready.
Gradually deletes old Pods while keeping the desired number available.

b) ReplicaSet During Updates
When Deployment is updated:
A new ReplicaSet is created with the updated Pod spec.
Kubernetes slowly scales up the new ReplicaSet while scaling down the old one.
For each new Pod that becomes Ready, one old Pod is deleted.

Pod in K8s

kubectl explain pod â†’ use this command to get the API version that your K8s supports.

YAML Syntax
spec.selector.matchLabels:

Tells the Deployment which Pods it should manage.
Like saying: â€œLook for Pods with these labels and treat them as part of this Deployment.â€

spec.template.metadata.labels:
Labels applied to the Pods when they are created.
Defines what labels each Pod will have.

Deployments / Replication Controller / Replicas
-------------------------------------------------------

ReplicationController ensures a specified number of Pods are always running.
It uses selector to manage Pods based on their labels.
It uses template.metadata.labels to apply labels to new Pods.

Updating a Deployment

When you change the Deployment (for example, updating the container image), Kubernetes handles it smoothly using Rolling Updates.
Rolling Update Process:
Kubernetes creates new Pods with the updated spec.
It waits for the new Pods to become ready.
It gradually deletes the old Pods while keeping the desired number of Pods available.
This continues until all old Pods are gone.
â¡ï¸ This process is called a Rolling Update.

Summary of Rollout Commands
----------------------
Command	Purpose
kubectl apply -f deployment.yaml	Apply or update Deployment
kubectl rollout status deployment my-app	Check rollout progress
kubectl rollout history deployment my-app	View rollout history
kubectl rollout undo deployment my-app	Rollback to previous version
kubectl rollout undo deployment my-app --to-revision=2	Rollback to a specific version
kubectl rollout pause deployment my-app	Pause rollout process
kubectl rollout resume deployment my-app	Resume rollout process
kubectl describe deployment my-app	View rollout details
kubectl rollout restart deployment my-app	Restart the Pods under deployment

Service Types
=------------------

ClusterIP
NodePort
LoadBalancer
ExternalName

# Example ExternalName service:

spec:
  selector:
    app: nginx-app
  type: ExternalName
  externalName: myapp.com
  ports:
  - port: 80
    targetPort: 80 

# Namespace

If a Pod is in a Namespace and another Pod is in a different Namespace, to connect them you must use the other Podâ€™s FQDN.

FQDN syntax:

**  service_name.namespace.svc.cluster.local

Example:
demo-service.demo_space.svc.cluster.local

##Sidecar vs Init Containers
---------------------------------
*Init Containers

If you add an init container in the Deployment, first the init container will start, then the main container will start.
Example:

initContainers:
- image: busybox:1.35
  imagePullPolicy: Never
  command: ['sh', '-c']
  args: ['echo Init setup']


âœ” Commands in args are executed inside the init containerâ€™s environment.
âœ” To share data with the main container, use shared volumes like emptyDir.
âœ” Once the init container finishes, the main container starts with whatever setup the init container performed.

Sidecar Containers
-----------------------------------------------
Run alongside the main container in the same Pod.

Share:
Network (same localhost).
Storage volumes (share data).

###Typical use cases:
Logging/monitoring agents.
Proxy containers (e.g., Envoy sidecar in Istio).
Updating or syncing configuration files.

DaemonSets
--------------------
Runs one Pod per Node.
Automatic behavior:
When new nodes are added, DaemonSet automatically adds the Pod.

###Use cases:
Monitoring agents (e.g., Prometheus Node Exporter).
Logging agents (e.g., Fluentd, Filebeat).
Network plugins (e.g., CNI plugins).
Security or backup tools.

CronJobs
Example:
* * * * * echo "Hello World" >> /home/yourusername/hello.log

Cron Syntax (5 fields):
Field	Meaning	Values
Minute	0â€“59	*
Hour	0â€“23	*
Day of Month	1â€“31	*
Month	1â€“12	*
Day of Week	0â€“6 (Sun=0)	*
Examples:

Run at 10 PM every Tuesday:
0 22 * * 2 echo "Hello World" >> /home/yourusername/hello.log
Run every 5 minutes:
*/5 * * * * echo "Hello World" >> /home/yourusername/hello.log

Where to use a CronJob:
--------------------------
âœ… Scheduled recurring tasks:
Backing up data every night at 2 AM.
Cleaning temporary files weekly.
Syncing data between services regularly.
âœ… Tasks that repeat and donâ€™t need manual intervention.

Jobs
------------------
Where to use a Job:

âœ… One-time tasks:
Database migration before deployment.
Processing a large batch of files once.
Sending a notification after an event.
âœ… Tasks that must complete and where you need to track success/failure.  

** Static Pods
------------------
A Static Pod is managed directly by the kubelet on a node (not by the API server).

***  Commonly used for control plane components:

/etc/kubernetes/manifests/kube-apiserver.yaml
/etc/kubernetes/manifests/kube-scheduler.yaml
/etc/kubernetes/manifests/kube-controller-manager.yaml
/etc/kubernetes/manifests/etcd.yaml

If you edit one of these files, kubelet automatically applies changes and restarts the Pod.

âœ… Summary:

âœ” Scheduler, API server, controller manager, etcd â†’ run as static pods.
âœ” Managed by kubelet, not the API server.
âœ” Always kept running on control plane nodes.
âœ” Critical for cluster functionality.

** Manual Scheduling
You explicitly tell Kubernetes where to run a Pod.
Options:
nodeName â†’ schedule Pod to a specific node.
nodeSelector / affinity â†’ more flexible placement rules.

** Useful when:
Strict control of Pod placement is required.
Specific hardware/resources needed.
Even if the scheduler is down, Pods can still be scheduled (since node placement is already defined).

Labels and Selectors
---------------------------------------
Selector = which Pods to manage.
Pod template labels = labels given to new Pods.
Must match â†’ otherwise Deployment wonâ€™t recognize its Pods.

Taints & Tolerations
--------------------------------------------
1ï¸âƒ£ Taints (on Nodes)

Prevent Pods from being scheduled on a node unless tolerated.
** Syntax:
key=value:effect

####Effects of taints:

NoSchedule â†’ Pods without a matching toleration will not be scheduled on this node.
PreferNoSchedule â†’ Scheduler will try to avoid placing pods on this node, but itâ€™s not guaranteed.
NoExecute â†’ Pods without a matching toleration will be evicted if already running, and new ones wonâ€™t be scheduled.

##   kubectl taint node node1 key=value:NoSchedule

â¡ï¸ Node node1 will not accept Pods unless they tolerate this taint.
2ï¸âƒ£ Tolerations (on Pods)
Allow a Pod to run on nodes with matching taints.
Example:

tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"

â¡ï¸ Pod can now be scheduled on nodes tainted with key=value:NoSchedule.

### NodeSelectors
-----------
You can add labels to nodes:
kubectl label node node_name key=value
Then in the Pod spec:

nodeSelector: 
  key: "value"

â¡ï¸ Pod will only be scheduled on nodes with the matching label.
NodeSelectors vs Taints/Tolerations
NodeSelectors â†’ â€œOnly run me on specific labeled nodes.â€
Taints & Tolerations â†’ â€œDonâ€™t run anything here unless it tolerates my taint.â€ 

Kubernetes Node Labels, Affinity & Tolerations
Node Labels â†’ Key/value pairs attached to nodes (e.g., disktype=ssd, env=prod). Used to group and identify nodes.

Add: kubectl label nodes <node-name> key=value
Remove: kubectl label nodes <node-name> key-

Node Affinity â†’ Pod-level rules that attract pods to specific nodes based on labels.
Required â†’ pod runs only on matching nodes.
Preferred â†’ pod tries to run on matching nodes but can fall back.
Taints & Tolerations â†’ Node-level rules that repel pods.
Taints block scheduling unless a pod has a matching toleration.

ğŸ‘‰ Difference:

Affinity = pod choosing nodes
Tolerations = nodes restricting pods
----
## REQUESTS AND LIMITS

1. Requests

This is the minimum amount of CPU and memory the container needs to start and run properly.
Kubernetes uses this to decide where to place (schedule) the pod.
ğŸŸ¢ Think: â€œGive me at least this much.â€

2. Limits

This is the maximum amount of CPU and memory the container can use.

If the container tries to use more than this:
For CPU: itâ€™s slowed down (throttled).
For Memory: it can be killed (OOMKilled).
ğŸ”´ Think: â€œDonâ€™t let me use more than this.â€

ğŸ§© Example

resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "500m"
-----------------------
### HPA & VPA

Scaling Types:
1 Horizontal autoscaler (also known as scale in/out)
    HPA changes the number of pods based on load.
    we do it on 2 different aspect first is workloads(like on pods{HPA}) and second is on infra(on nodes{CLUSTER autoscaler})
    
 >>>  kubectl autoscale deployment myapp --cpu-percent=70 --min=3 --max=10
  âœ… When to use HPA:
  You want to handle varying traffic.
  Your app can run multiple replicas easily (stateless apps).

2 Vertical autoscaler (also known as scale up/down)
   VPA changes the resources (CPU & memory) given to each pod â€” not the number of pods.
  ğŸ“ˆ If your app needs more power â†’ adds CPU/memory
  ğŸ“‰ If it needs less â†’ reduces CPU/memory
   we do it on 2 different aspect first is workloads(like on pods{HPA}) and second is on infra(on nodes{CLUSTER autoscaler})

  âœ… When to use VPA:
    
    You have a few pods that must stay single (like a database).
    Your app is stateful or not easily scaled horizontally.
    You want to optimize resource usage automatically.

âœ… Health Probes ###
----------------------


âœ… configmaps and secrets :
---------
configmap: 
 you can use the configmaps as environment variable and as volume mounts
 to create and add as a volume mount create the configmap and in deployment yaml  add volumeMounts and volume
 ex:  
  spec: 
    containers:
    -  name: dummy-app
       image: python3:1.0
       volumeMounts: 
       - name: my-db-vol   ## name is same as the volume
         mountPath: /opt
    volumes:
    -  name: my-db-vol    ## volume name 
       ConfigMap:
          name: test-cm   ## Configmap name

$$$ Secrets:
------------
imperative way :
   kubectl create secret generic db-secret \
  --from-literal=username=admin \
  --from-literal=password=MyP@ssw0rd
Declarative way :
----------
      apiVersion: v1
      kind: Secret
      metadata:
        name: db-secret
      type: Opaque
      data:
        username: YWRtaW4=        # base64 for "admin"
        password: TXlQQHNzdzByZA==  # base64 for "MyP@ssw0rd"


how to use secret in deployment yaml:
 -----------
  spec:
  containers:
    - name: demo-container
      image: nginx
      env:
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
  

ğŸ”’CERTIFICATES IN KUBERNETES ğŸ”’
-------------------------------

ğŸ”‘ how ssl/Tls works: 
----
* symmetric encyption , asymmetric encryption

When a client sends a GET request over HTTPS:
The server has a public and private key pair.
The server sends its public key (inside a certificate) to the client.
The client verifies the certificate and then generates a random pre-master secret.
The client encrypts this secret using the serverâ€™s public key and sends it.
The server decrypts it with its private key.
Both sides now use that secret to create a shared session key for encrypted communication.

### Manage TLS Certificates In a Kubernetes Cluster - Create Certificate Signing Request:
----------
public certificate : anything that has .crt & .pem as an extention is a public certificate . ex :  server.crt,server.pem,client.crt,client.pem
Private Key : anything that has key in the name or extention is private key . ex: server.key, server-key.pem 

Create user certificates :
  1.  openssl genrsa -out user.key 2048 (this command will generate the private key for user )
  2. openssl req -new -key user.key -out user.csr -subj "/CN=username" (User generates a Certificate Signing Request (CSR))

after these 2 steps you will get user.csr and user.key files 

  3. User sends the CSR to the Admin :
The user sends the CSR to the administrator who will be responsible for approving the request and signing the certificate.
4. create a csr.yaml so that when you apply that yaml it will create a scr request object .


apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: user-csr
spec:
  request: <base64-encoded-csr>
  signerName: kubernetes.io/kubelet-apiserver-client
  usages:
  - digital signature
  - key encipherment
  - client auth

here in the request section you have to paste the csr data which is base64 encoded ,to convert the data to base64 use this command , whcih will turn 
the csr to base64 format and print that in one line , paste that in that request section in the yaml .

>>>  cat user.csr | base64 | tr -d "\n"

after this you apply the yaml (kubectl apply user-csr.yaml) can check by : kubectl get csr 
The request field should be set to the base64-encoded CSR generated by the user. The signerName field should be set to the name of the certificate signer that will be used to sign the certificate. The usages field should be set to the list of intended usages for the certificate, which in this case includes digital signature, key encipherment, and client authentication.
it will give the csr object and it might show as pending , cause its not approved by any CA authority yet ,

5. Admin approves or rejects the CSR :
  get list of csr :  kubectl get csr
  approve csr :  kubectl certificate approve user-csr
  deny the csr :   kubectl certificate deny user-csr


The kube-controller-manager detects the approval.
It signs the CSR using the clusterâ€™s CA key and certificate.
The signed certificate is stored back in the CSRâ€™s status.certificate field.
You can extract the signed certificate as follows:

>>>>>> kubectl get csr <csr-name> -o jsonpath='{.status.certificate}' | base64 -d > client.crt

6............Cluster CA Configuration
The Kubernetes cluster CA is managed by the kube-controller-manager.
The CAâ€™s key and certificate are usually located on the control plane node(s):
              /etc/kubernetes/pki/ca.crt
              /etc/kubernetes/pki/ca.key
The controller manager uses these flags to reference them:
          --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
          --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
If these files or flags are missing, approved CSRs will not result in signed certificates

âœ… Kubernetes Authentication and Authorization :
------
SO when we use any kubectl command to interact with the server somehow it knows what permissions we have and based on that it is showing the results .

Types of authorization:
-----
| Type        | Description                       | Common Use                        | Managed By        |
| ----------- | --------------------------------- | --------------------------------- | ----------------- |
| **RBAC**    | Role-based access control         | Default choice for most clusters  | Cluster Admin     |
| **ABAC**    | Attribute-based (JSON policies)   | Legacy/testing                    | Cluster Admin     |
| **Node**    | Limits node (Kubelet) permissions | Internal system use               | Kubernetes itself |
| **Webhook** | Delegates to external service     | Custom or enterprise auth systems | External service  |


âœ… Role Based Access Control Kubernetes: 
------
kubernetes does not deal with user management , it offloads it to identity providers like : aws IAM, LDAP, Azure active Dairectory
RBAC (Role-Based Access Control) controls who can do what within a Kubernetes cluster.

It uses four key objects: Role, ClusterRole, RoleBinding ,ClusterRoleBinding
ğŸ”¸ 1. Role : A Role defines a set of permissions (like read, write, delete) for resources within a specific namespace.
ğŸ”¸ 2. ClusterRole: A ClusterRole is similar to a Role â€” but itâ€™s cluster-wide.
    It can:
    Grant permissions across all namespaces, or
    Be used for non-namespaced resources (like nodes, persistent volumes, etc.)

RoleBinding â†’ connects a Role to a user or service account in a namespace
ClusterRoleBinding â†’ connects a ClusterRole to a user or service account cluster-wide

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: development
rules: 
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
##  here in role yaml you have to mentiones rules: apigroups , resources(that you want to add for the role), verbs (here you add what kind of access you want )

### Rolebinding:
apiVersion: rbac.authorization.k8s.io/v1
Kind: RoleBinding
metadata: 
   name: read-pods-binding
   namespace: development
subjects:
- Kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io 
RoleRef:
- kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

## in RoleBinding you have to add subjects: under that add - kind:User, name of user, apigroup that is you added in the apiVersion
next add RoleRef , this is the role you want to bind with the user, in this add Roleref: kind: Role, name of the role you have created,
apiGroup
### Cluster Role and Role binding: this is for the entire cluster , you shouldnot mention namespace in the defining yaml .

ğŸ“¦ Core Group and Not Core group :
1ï¸âƒ£ Core group :
The oldest, built-in resources in Kubernetes.
They donâ€™t have a group name â€” theyâ€™re just part of the main API.
When writing RBAC, you show this with apiGroups: [""] (empty string).
ğŸ“¦ Examples of core group resources:  Pods,Services,ConfigMaps,Secrets,Namespaces,Nodes,PersistentVolumeClaims

ğŸ”¹ 2ï¸âƒ£ Non-core (named) groups:
Newer or specialized features are put in separate groups.
Each one has a group name like apps, batch, networking.k8s.io, etc.
In RBAC, you write that name.

| Resource               | API group                 | How to write in RBAC                       |
| ---------------------- | ------------------------- | ------------------------------------------ |
| Deployments            | apps                      | `apiGroups: ["apps"]`                      |
| Jobs, CronJobs         | batch                     | `apiGroups: ["batch"]`                     |
| Roles, ClusterRoles    | rbac.authorization.k8s.io | `apiGroups: ["rbac.authorization.k8s.io"]` |
| Ingress, NetworkPolicy | networking.k8s.io         | `apiGroups: ["networking.k8s.io"]`         |


####  and User and Groups belongs to the api group 

User and Group objects are not real Kubernetes resources in the API server.
They are identities managed outside Kubernetes (like in kubeconfig or an identity provider).
Kubernetes defines these kinds (User, Group) within the RBAC API group â€” hence rbac.authorization.k8s.io.

| Subject kind       | Example                           | Correct `apiGroup` value    |
| ------------------ | --------------------------------- | --------------------------- |
| **User**           | `name: alice`                     | `rbac.authorization.k8s.io` |
| **Group**          | `name: dev-team`                  | `rbac.authorization.k8s.io` |
| **ServiceAccount** | `name: app-sa`, `namespace: prod` | `""` *(empty string)*       |

| Location                     | Key         | Example | Notes                       |
| ---------------------------- | ----------- | ------- | --------------------------- |
| In `rules:`                  | `apiGroups` | `[""]`  | List â€” for resource groups  |
| In `subjects:` or `roleRef:` | `apiGroup`  | `""`    | String â€” for RBAC API group |

ğŸ§© Rule of Thumb
â¡ï¸ Roles are namespaced objects, so if you donâ€™t include it, the Role might not apply where you expect.
   A single Role can have multiple rules: blocks,
   and each block can define access to one or more resources (pods, configmaps, secrets, etc.).
   Then you bind that one Role to a User, Group, or ServiceAccount using a single RoleBinding.
â¡ï¸ Each Role needs its own RoleBinding,
even if youâ€™re binding them to the same ServiceAccount, User, or Group.
ğŸ§  Why?
Because:
A RoleBinding connects one Role to one or more subjects.
It doesnâ€™t support binding multiple Roles at once.
So if your ServiceAccount needs permissions from two different Roles,
â†’ you just create two RoleBindings, both pointing to the same ServiceAccount

In RBAC, main resources and subresources are treated separately.
So if you want to do something like kubectl logs or kubectl exec,
you must give permissions for the subresource â€” not just the main resource
âœ… service accounts:
--------------------
A ServiceAccount is an identity for pods.
Pods use it when they need to talk to the Kubernetes API (like listing pods, reading ConfigMaps, etc.).
By default, every pod uses the default ServiceAccount, which has no special permissions.
You create your own ServiceAccount when your app needs specific access.
You then bind Roles or ClusterRoles to it using RoleBinding or ClusterRoleBinding.
This gives the pod only the permissions you choose â€” nothing more.

apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-serviceaccount
  namespace: default

>>>>>  to attach a Pod to SA you need to add ""ServiceAccountName"" in Pod specs in deployment yaml

ğŸ§  3ï¸âƒ£ Why use your own ServiceAccount anyway?

Even though pods â€œworkâ€ with the default SA, using a custom one is best practice because:
You can give only the exact permissions that pod needs.
You avoid accidentally giving too much power to all pods using the default SA.
Itâ€™s safer and easier to audit later.

ğŸ§  SubResource in RBAC :
---------------------------------------
In RBAC, main resources and subresources are treated separately.
So if you want to do something like kubectl logs or kubectl exec,
you must give permissions for the subresource â€” not just the main resource.
ğŸ‘‰ If your kubectl command does an action on a resource instead of editing it directly,
itâ€™s probably a subresource.

If your command involves logs, exec, attach, scale, status, or approval â†’
itâ€™s a subresource, so add it as "pods/log" or "deployments/scale" in your RBAC resources: list.

âš¡ In short
Subresources = â€œspecial API actionsâ€ related to a resource.
RBAC needs explicit permission for them.
Trick: if your kubectl command does something to the resource (not just gets it),
itâ€™s probably a subresource.

âœ…Kubernetes Network Policies :
----------------
CNI (container network interface) :
-----
Itâ€™s a plugin system that tells Kubernetes how Pods get network connectivity â€”
how they get IPs, talk to each other, and reach the outside world.

Think of CNI as the network driver for your 


âœ… Custom resource definition (CRD)
----------------------------


