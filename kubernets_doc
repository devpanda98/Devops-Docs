Kubernetes Notes
1. Pods in Kubernetes

Pod: The smallest deployable unit in Kubernetes.
Defined using YAML manifest files.
You can check supported API versions with:
kubectl explain pod

2. YAML Syntax (Key Concepts)

a) spec.selector.matchLabels
Defines which Pods a Deployment should manage.
Works like a filter: ‚ÄúSelect Pods with these labels.‚Äù
b) spec.template.metadata.labels
Labels that are actually applied to Pods when created.
Defines what labels each Pod will have.

3. Deployments & Replica Management

a) ReplicationController
Ensures a specified number of Pods are always running.
Uses:
Selector ‚Üí to match and manage Pods by labels.
template.metadata.labels ‚Üí applies labels to new Pods.

b) Deployment
Manages ReplicaSets (which in turn manage Pods).
Provides features like updates, rollbacks, and scaling.

4. Updating a Deployment

a) Rolling Updates (Default Strategy)
Ensures zero downtime updates.
Process:
Kubernetes creates new Pods with the updated spec (e.g., new image).
Waits until new Pods are Ready.
Gradually deletes old Pods while keeping the desired number available.

b) ReplicaSet During Updates
When Deployment is updated:
A new ReplicaSet is created with the updated Pod spec.
Kubernetes slowly scales up the new ReplicaSet while scaling down the old one.
For each new Pod that becomes Ready, one old Pod is deleted.

Pod in K8s

kubectl explain pod ‚Üí use this command to get the API version that your K8s supports.

YAML Syntax
spec.selector.matchLabels:

Tells the Deployment which Pods it should manage.
Like saying: ‚ÄúLook for Pods with these labels and treat them as part of this Deployment.‚Äù

spec.template.metadata.labels:
Labels applied to the Pods when they are created.
Defines what labels each Pod will have.

Deployments / Replication Controller / Replicas
-------------------------------------------------------

ReplicationController ensures a specified number of Pods are always running.
It uses selector to manage Pods based on their labels.
It uses template.metadata.labels to apply labels to new Pods.

Updating a Deployment

When you change the Deployment (for example, updating the container image), Kubernetes handles it smoothly using Rolling Updates.
Rolling Update Process:
Kubernetes creates new Pods with the updated spec.
It waits for the new Pods to become ready.
It gradually deletes the old Pods while keeping the desired number of Pods available.
This continues until all old Pods are gone.
‚û°Ô∏è This process is called a Rolling Update.

Summary of Rollout Commands
----------------------
Command	Purpose
kubectl apply -f deployment.yaml	Apply or update Deployment
kubectl rollout status deployment my-app	Check rollout progress
kubectl rollout history deployment my-app	View rollout history
kubectl rollout undo deployment my-app	Rollback to previous version
kubectl rollout undo deployment my-app --to-revision=2	Rollback to a specific version
kubectl rollout pause deployment my-app	Pause rollout process
kubectl rollout resume deployment my-app	Resume rollout process
kubectl describe deployment my-app	View rollout details
kubectl rollout restart deployment my-app	Restart the Pods under deployment

Service Types
=------------------

ClusterIP
NodePort
LoadBalancer
ExternalName

# Example ExternalName service:

spec:
  selector:
    app: nginx-app
  type: ExternalName
  externalName: myapp.com
  ports:
  - port: 80
    targetPort: 80 

# Namespace

If a Pod is in a Namespace and another Pod is in a different Namespace, to connect them you must use the other Pod‚Äôs FQDN.

FQDN syntax:

**  service_name.namespace.svc.cluster.local

Example:
demo-service.demo_space.svc.cluster.local

##Sidecar vs Init Containers
---------------------------------
*Init Containers

If you add an init container in the Deployment, first the init container will start, then the main container will start.
Example:

initContainers:
- image: busybox:1.35
  imagePullPolicy: Never
  command: ['sh', '-c']
  args: ['echo Init setup']


‚úî Commands in args are executed inside the init container‚Äôs environment.
‚úî To share data with the main container, use shared volumes like emptyDir.
‚úî Once the init container finishes, the main container starts with whatever setup the init container performed.

Sidecar Containers
-----------------------------------------------
Run alongside the main container in the same Pod.

Share:
Network (same localhost).
Storage volumes (share data).

###Typical use cases:
Logging/monitoring agents.
Proxy containers (e.g., Envoy sidecar in Istio).
Updating or syncing configuration files.

DaemonSets
--------------------
Runs one Pod per Node.
Automatic behavior:
When new nodes are added, DaemonSet automatically adds the Pod.

###Use cases:
Monitoring agents (e.g., Prometheus Node Exporter).
Logging agents (e.g., Fluentd, Filebeat).
Network plugins (e.g., CNI plugins).
Security or backup tools.

CronJobs
Example:
* * * * * echo "Hello World" >> /home/yourusername/hello.log

Cron Syntax (5 fields):
Field	Meaning	Values
Minute	0‚Äì59	*
Hour	0‚Äì23	*
Day of Month	1‚Äì31	*
Month	1‚Äì12	*
Day of Week	0‚Äì6 (Sun=0)	*
Examples:

Run at 10 PM every Tuesday:
0 22 * * 2 echo "Hello World" >> /home/yourusername/hello.log
Run every 5 minutes:
*/5 * * * * echo "Hello World" >> /home/yourusername/hello.log

Where to use a CronJob:
--------------------------
‚úÖ Scheduled recurring tasks:
Backing up data every night at 2 AM.
Cleaning temporary files weekly.
Syncing data between services regularly.
‚úÖ Tasks that repeat and don‚Äôt need manual intervention.

Jobs
------------------
Where to use a Job:

‚úÖ One-time tasks:
Database migration before deployment.
Processing a large batch of files once.
Sending a notification after an event.
‚úÖ Tasks that must complete and where you need to track success/failure.  

** Static Pods
------------------
A Static Pod is managed directly by the kubelet on a node (not by the API server).

***  Commonly used for control plane components:

/etc/kubernetes/manifests/kube-apiserver.yaml
/etc/kubernetes/manifests/kube-scheduler.yaml
/etc/kubernetes/manifests/kube-controller-manager.yaml
/etc/kubernetes/manifests/etcd.yaml

If you edit one of these files, kubelet automatically applies changes and restarts the Pod.

‚úÖ Summary:

‚úî Scheduler, API server, controller manager, etcd ‚Üí run as static pods.
‚úî Managed by kubelet, not the API server.
‚úî Always kept running on control plane nodes.
‚úî Critical for cluster functionality.

** Manual Scheduling
You explicitly tell Kubernetes where to run a Pod.
Options:
nodeName ‚Üí schedule Pod to a specific node.
nodeSelector / affinity ‚Üí more flexible placement rules.

** Useful when:
Strict control of Pod placement is required.
Specific hardware/resources needed.
Even if the scheduler is down, Pods can still be scheduled (since node placement is already defined).

Labels and Selectors
---------------------------------------
Selector = which Pods to manage.
Pod template labels = labels given to new Pods.
Must match ‚Üí otherwise Deployment won‚Äôt recognize its Pods.

Taints & Tolerations
--------------------------------------------
1Ô∏è‚É£ Taints (on Nodes)

Prevent Pods from being scheduled on a node unless tolerated.
** Syntax:
key=value:effect

####Effects of taints:

NoSchedule ‚Üí Pods without a matching toleration will not be scheduled on this node.
PreferNoSchedule ‚Üí Scheduler will try to avoid placing pods on this node, but it‚Äôs not guaranteed.
NoExecute ‚Üí Pods without a matching toleration will be evicted if already running, and new ones won‚Äôt be scheduled.

##   kubectl taint node node1 key=value:NoSchedule

‚û°Ô∏è Node node1 will not accept Pods unless they tolerate this taint.
2Ô∏è‚É£ Tolerations (on Pods)
Allow a Pod to run on nodes with matching taints.
Example:

tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"

‚û°Ô∏è Pod can now be scheduled on nodes tainted with key=value:NoSchedule.

### NodeSelectors
-----------
You can add labels to nodes:
kubectl label node node_name key=value
Then in the Pod spec:

nodeSelector: 
  key: "value"

‚û°Ô∏è Pod will only be scheduled on nodes with the matching label.
NodeSelectors vs Taints/Tolerations
NodeSelectors ‚Üí ‚ÄúOnly run me on specific labeled nodes.‚Äù
Taints & Tolerations ‚Üí ‚ÄúDon‚Äôt run anything here unless it tolerates my taint.‚Äù 

Kubernetes Node Labels, Affinity & Tolerations
Node Labels ‚Üí Key/value pairs attached to nodes (e.g., disktype=ssd, env=prod). Used to group and identify nodes.

Add: kubectl label nodes <node-name> key=value
Remove: kubectl label nodes <node-name> key-

Node Affinity ‚Üí Pod-level rules that attract pods to specific nodes based on labels.
Required ‚Üí pod runs only on matching nodes.
Preferred ‚Üí pod tries to run on matching nodes but can fall back.
Taints & Tolerations ‚Üí Node-level rules that repel pods.
Taints block scheduling unless a pod has a matching toleration.

üëâ Difference:

Affinity = pod choosing nodes
Tolerations = nodes restricting pods
----
## REQUESTS AND LIMITS

1. Requests

This is the minimum amount of CPU and memory the container needs to start and run properly.
Kubernetes uses this to decide where to place (schedule) the pod.
üü¢ Think: ‚ÄúGive me at least this much.‚Äù

2. Limits

This is the maximum amount of CPU and memory the container can use.

If the container tries to use more than this:
For CPU: it‚Äôs slowed down (throttled).
For Memory: it can be killed (OOMKilled).
üî¥ Think: ‚ÄúDon‚Äôt let me use more than this.‚Äù

üß© Example

resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "500m"
-----------------------
### HPA & VPA

Scaling Types:
1 Horizontal autoscaler (also known as scale in/out)
    HPA changes the number of pods based on load.
    we do it on 2 different aspect first is workloads(like on pods{HPA}) and second is on infra(on nodes{CLUSTER autoscaler})
    
 >>>  kubectl autoscale deployment myapp --cpu-percent=70 --min=3 --max=10
  ‚úÖ When to use HPA:
  You want to handle varying traffic.
  Your app can run multiple replicas easily (stateless apps).

2 Vertical autoscaler (also known as scale up/down)
   VPA changes the resources (CPU & memory) given to each pod ‚Äî not the number of pods.
  üìà If your app needs more power ‚Üí adds CPU/memory
  üìâ If it needs less ‚Üí reduces CPU/memory
   we do it on 2 different aspect first is workloads(like on pods{HPA}) and second is on infra(on nodes{CLUSTER autoscaler})

  ‚úÖ When to use VPA:
    
    You have a few pods that must stay single (like a database).
    Your app is stateful or not easily scaled horizontally.
    You want to optimize resource usage automatically.

‚úÖ Health Probes ###
----------------------


‚úÖ configmaps and secrets :
---------
configmap: 
 you can use the configmaps as environment variable and as volume mounts
 to create and add as a volume mount create the configmap and in deployment yaml  add volumeMounts and volume
 ex:  
  spec: 
    containers:
    -  name: dummy-app
       image: python3:1.0
       volumeMounts: 
       - name: my-db-vol   ## name is same as the volume
         mountPath: /opt
    volumes:
    -  name: my-db-vol    ## volume name 
       ConfigMap:
          name: test-cm   ## Configmap name

$$$ Secrets:
------------
imperative way :
   kubectl create secret generic db-secret \
  --from-literal=username=admin \
  --from-literal=password=MyP@ssw0rd
Declarative way :
----------
      apiVersion: v1
      kind: Secret
      metadata:
        name: db-secret
      type: Opaque
      data:
        username: YWRtaW4=        # base64 for "admin"
        password: TXlQQHNzdzByZA==  # base64 for "MyP@ssw0rd"


how to use secret in deployment yaml:
 -----------
  spec:
  containers:
    - name: demo-container
      image: nginx
      env:
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
  

üîíCERTIFICATES IN KUBERNETES üîí
-------------------------------

üîë how ssl/Tls works: 
----
* symmetric encyption , asymmetric encryption

When a client sends a GET request over HTTPS:
The server has a public and private key pair.
The server sends its public key (inside a certificate) to the client.
The client verifies the certificate and then generates a random pre-master secret.
The client encrypts this secret using the server‚Äôs public key and sends it.
The server decrypts it with its private key.
Both sides now use that secret to create a shared session key for encrypted communication.

### Manage TLS Certificates In a Kubernetes Cluster - Create Certificate Signing Request:
----------
public certificate : anything that has .crt & .pem as an extention is a public certificate . ex :  server.crt,server.pem,client.crt,client.pem
Private Key : anything that has key in the name or extention is private key . ex: server.key, server-key.pem 

Create user certificates :
  1.  openssl genrsa -out user.key 2048 (this command will generate the private key for user )
  2. openssl req -new -key user.key -out user.csr -subj "/CN=username" (User generates a Certificate Signing Request (CSR))

after these 2 steps you will get user.csr and user.key files 

  3. User sends the CSR to the Admin :
The user sends the CSR to the administrator who will be responsible for approving the request and signing the certificate.
4. create a csr.yaml so that when you apply that yaml it will create a scr request object .


apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: user-csr
spec:
  request: <base64-encoded-csr>
  signerName: kubernetes.io/kubelet-apiserver-client
  usages:
  - digital signature
  - key encipherment
  - client auth

here in the request section you have to paste the csr data which is base64 encoded ,to convert the data to base64 use this command , whcih will turn 
the csr to base64 format and print that in one line , paste that in that request section in the yaml .

>>>  cat user.csr | base64 | tr -d "\n"

after this you apply the yaml (kubectl apply user-csr.yaml) can check by : kubectl get csr 
The request field should be set to the base64-encoded CSR generated by the user. The signerName field should be set to the name of the certificate signer that will be used to sign the certificate. The usages field should be set to the list of intended usages for the certificate, which in this case includes digital signature, key encipherment, and client authentication.
it will give the csr object and it might show as pending , cause its not approved by any CA authority yet ,

5. Admin approves or rejects the CSR :
  get list of csr :  kubectl get csr
  approve csr :  kubectl certificate approve user-csr
  deny the csr :   kubectl certificate deny user-csr


The kube-controller-manager detects the approval.
It signs the CSR using the cluster‚Äôs CA key and certificate.
The signed certificate is stored back in the CSR‚Äôs status.certificate field.
You can extract the signed certificate as follows:

>>>>>> kubectl get csr <csr-name> -o jsonpath='{.status.certificate}' | base64 -d > client.crt

6............Cluster CA Configuration
The Kubernetes cluster CA is managed by the kube-controller-manager.
The CA‚Äôs key and certificate are usually located on the control plane node(s):
              /etc/kubernetes/pki/ca.crt
              /etc/kubernetes/pki/ca.key
The controller manager uses these flags to reference them:
          --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
          --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
If these files or flags are missing, approved CSRs will not result in signed certificates

‚úÖ Kubernetes Authentication and Authorization :
------
SO when we use any kubectl command to interact with the server somehow it knows what permissions we have and based on that it is showing the results .

Types of authorization:
-----
| Type        | Description                       | Common Use                        | Managed By        |
| ----------- | --------------------------------- | --------------------------------- | ----------------- |
| **RBAC**    | Role-based access control         | Default choice for most clusters  | Cluster Admin     |
| **ABAC**    | Attribute-based (JSON policies)   | Legacy/testing                    | Cluster Admin     |
| **Node**    | Limits node (Kubelet) permissions | Internal system use               | Kubernetes itself |
| **Webhook** | Delegates to external service     | Custom or enterprise auth systems | External service  |


‚úÖ Role Based Access Control Kubernetes: 
------
kubernetes does not deal with user management , it offloads it to identity providers like : aws IAM, LDAP, Azure active Dairectory
RBAC (Role-Based Access Control) controls who can do what within a Kubernetes cluster.

It uses four key objects: Role, ClusterRole, RoleBinding ,ClusterRoleBinding
üî∏ 1. Role : A Role defines a set of permissions (like read, write, delete) for resources within a specific namespace.
üî∏ 2. ClusterRole: A ClusterRole is similar to a Role ‚Äî but it‚Äôs cluster-wide.
    It can:
    Grant permissions across all namespaces, or
    Be used for non-namespaced resources (like nodes, persistent volumes, etc.)

RoleBinding ‚Üí connects a Role to a user or service account in a namespace
ClusterRoleBinding ‚Üí connects a ClusterRole to a user or service account cluster-wide


apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: development
rules: 
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
##  here in role yaml you have to mentiones rules: apigroups , resources(that you want to add for the role), verbs (here you add what kind of access you want )

### Rolebinding:
 
apiVersion: rbac.authorization.k8s.io/v1
Kind: RoleBinding
metadata: 
   name: read-pods-binding
   namespace: development
subjects:
- Kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io 
RoleRef:
- kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

## in RoleBinding you have to add subjects: under that add - kind:User, name of user, apigroup that is you added in the apiVersion
next add RoleRef , this is the role you want to bind with the user, in this add Roleref: kind: Role, name of the role you have created,
apiGroup


### Cluster Role and Role binding:
 -----------



‚úÖ service accounts:
--------------------





‚úÖKubernetes Network Policies :
----------------
CNI (container network interface) :
-----
It‚Äôs a plugin system that tells Kubernetes how Pods get network connectivity ‚Äî
how they get IPs, talk to each other, and reach the outside world.

Think of CNI as the network driver for your 


‚úÖ Custom resource definition (CRD)
----------------------------


