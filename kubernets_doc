Kubernetes Notes
1. Pods in Kubernetes

Pod: The smallest deployable unit in Kubernetes.
Defined using YAML manifest files.
You can check supported API versions with:
kubectl explain pod

2. YAML Syntax (Key Concepts)

a) spec.selector.matchLabels
Defines which Pods a Deployment should manage.
Works like a filter: ‚ÄúSelect Pods with these labels.‚Äù
b) spec.template.metadata.labels
Labels that are actually applied to Pods when created.
Defines what labels each Pod will have.

3. Deployments & Replica Management

a) ReplicationController
Ensures a specified number of Pods are always running.
Uses:
Selector ‚Üí to match and manage Pods by labels.
template.metadata.labels ‚Üí applies labels to new Pods.

b) Deployment
Manages ReplicaSets (which in turn manage Pods).
Provides features like updates, rollbacks, and scaling.

4. Updating a Deployment

a) Rolling Updates (Default Strategy)
Ensures zero downtime updates.
Process:
Kubernetes creates new Pods with the updated spec (e.g., new image).
Waits until new Pods are Ready.
Gradually deletes old Pods while keeping the desired number available.

b) ReplicaSet During Updates
When Deployment is updated:
A new ReplicaSet is created with the updated Pod spec.
Kubernetes slowly scales up the new ReplicaSet while scaling down the old one.
For each new Pod that becomes Ready, one old Pod is deleted.

Pod in K8s

kubectl explain pod ‚Üí use this command to get the API version that your K8s supports.

YAML Syntax
spec.selector.matchLabels:

Tells the Deployment which Pods it should manage.
Like saying: ‚ÄúLook for Pods with these labels and treat them as part of this Deployment.‚Äù

spec.template.metadata.labels:
Labels applied to the Pods when they are created.
Defines what labels each Pod will have.

Deployments / Replication Controller / Replicas
-------------------------------------------------------

ReplicationController ensures a specified number of Pods are always running.
It uses selector to manage Pods based on their labels.
It uses template.metadata.labels to apply labels to new Pods.

Updating a Deployment

When you change the Deployment (for example, updating the container image), Kubernetes handles it smoothly using Rolling Updates.
Rolling Update Process:
Kubernetes creates new Pods with the updated spec.
It waits for the new Pods to become ready.
It gradually deletes the old Pods while keeping the desired number of Pods available.
This continues until all old Pods are gone.
‚û°Ô∏è This process is called a Rolling Update.

Summary of Rollout Commands
----------------------
Command	Purpose
kubectl apply -f deployment.yaml	Apply or update Deployment
kubectl rollout status deployment my-app	Check rollout progress
kubectl rollout history deployment my-app	View rollout history
kubectl rollout undo deployment my-app	Rollback to previous version
kubectl rollout undo deployment my-app --to-revision=2	Rollback to a specific version
kubectl rollout pause deployment my-app	Pause rollout process
kubectl rollout resume deployment my-app	Resume rollout process
kubectl describe deployment my-app	View rollout details
kubectl rollout restart deployment my-app	Restart the Pods under deployment

Service Types
=------------------

ClusterIP
NodePort
LoadBalancer
ExternalName

# Example ExternalName service:

spec:
  selector:
    app: nginx-app
  type: ExternalName
  externalName: myapp.com
  ports:
  - port: 80
    targetPort: 80 

# Namespace

If a Pod is in a Namespace and another Pod is in a different Namespace, to connect them you must use the other Pod‚Äôs FQDN.

FQDN syntax:

**  service_name.namespace.svc.cluster.local

Example:
demo-service.demo_space.svc.cluster.local

##Sidecar vs Init Containers
---------------------------------
*Init Containers

If you add an init container in the Deployment, first the init container will start, then the main container will start.
Example:

initContainers:
- image: busybox:1.35
  imagePullPolicy: Never
  command: ['sh', '-c']
  args: ['echo Init setup']


‚úî Commands in args are executed inside the init container‚Äôs environment.
‚úî To share data with the main container, use shared volumes like emptyDir.
‚úî Once the init container finishes, the main container starts with whatever setup the init container performed.

Sidecar Containers
-----------------------------------------------
Run alongside the main container in the same Pod.

Share:
Network (same localhost).
Storage volumes (share data).

###Typical use cases:
Logging/monitoring agents.
Proxy containers (e.g., Envoy sidecar in Istio).
Updating or syncing configuration files.

DaemonSets
--------------------
Runs one Pod per Node.
Automatic behavior:
When new nodes are added, DaemonSet automatically adds the Pod.

###Use cases:
Monitoring agents (e.g., Prometheus Node Exporter).
Logging agents (e.g., Fluentd, Filebeat).
Network plugins (e.g., CNI plugins).
Security or backup tools.

CronJobs
Example:
* * * * * echo "Hello World" >> /home/yourusername/hello.log

Cron Syntax (5 fields):
Field	Meaning	Values
Minute	0‚Äì59	*
Hour	0‚Äì23	*
Day of Month	1‚Äì31	*
Month	1‚Äì12	*
Day of Week	0‚Äì6 (Sun=0)	*
Examples:

Run at 10 PM every Tuesday:
0 22 * * 2 echo "Hello World" >> /home/yourusername/hello.log
Run every 5 minutes:
*/5 * * * * echo "Hello World" >> /home/yourusername/hello.log

Where to use a CronJob:
--------------------------
‚úÖ Scheduled recurring tasks:
Backing up data every night at 2 AM.
Cleaning temporary files weekly.
Syncing data between services regularly.
‚úÖ Tasks that repeat and don‚Äôt need manual intervention.

Jobs
------------------
Where to use a Job:

‚úÖ One-time tasks:
Database migration before deployment.
Processing a large batch of files once.
Sending a notification after an event.
‚úÖ Tasks that must complete and where you need to track success/failure.  

** Static Pods
------------------
A Static Pod is managed directly by the kubelet on a node (not by the API server).

***  Commonly used for control plane components:

/etc/kubernetes/manifests/kube-apiserver.yaml
/etc/kubernetes/manifests/kube-scheduler.yaml
/etc/kubernetes/manifests/kube-controller-manager.yaml
/etc/kubernetes/manifests/etcd.yaml

If you edit one of these files, kubelet automatically applies changes and restarts the Pod.

‚úÖ Summary:

‚úî Scheduler, API server, controller manager, etcd ‚Üí run as static pods.
‚úî Managed by kubelet, not the API server.
‚úî Always kept running on control plane nodes.
‚úî Critical for cluster functionality.

** Manual Scheduling
You explicitly tell Kubernetes where to run a Pod.
Options:
nodeName ‚Üí schedule Pod to a specific node.
nodeSelector / affinity ‚Üí more flexible placement rules.

** Useful when:
Strict control of Pod placement is required.
Specific hardware/resources needed.
Even if the scheduler is down, Pods can still be scheduled (since node placement is already defined).

Labels and Selectors
---------------------------------------
Selector = which Pods to manage.
Pod template labels = labels given to new Pods.
Must match ‚Üí otherwise Deployment won‚Äôt recognize its Pods.

Taints & Tolerations
--------------------------------------------
1Ô∏è‚É£ Taints (on Nodes)

Prevent Pods from being scheduled on a node unless tolerated.
** Syntax:
key=value:effect

####Effects of taints:

NoSchedule ‚Üí Pods without a matching toleration will not be scheduled on this node.
PreferNoSchedule ‚Üí Scheduler will try to avoid placing pods on this node, but it‚Äôs not guaranteed.
NoExecute ‚Üí Pods without a matching toleration will be evicted if already running, and new ones won‚Äôt be scheduled.

##   kubectl taint node node1 key=value:NoSchedule

‚û°Ô∏è Node node1 will not accept Pods unless they tolerate this taint.
2Ô∏è‚É£ Tolerations (on Pods)
Allow a Pod to run on nodes with matching taints.
Example:

tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"

‚û°Ô∏è Pod can now be scheduled on nodes tainted with key=value:NoSchedule.

### NodeSelectors
-----------
You can add labels to nodes:
kubectl label node node_name key=value
Then in the Pod spec:

nodeSelector: 
  key: "value"

‚û°Ô∏è Pod will only be scheduled on nodes with the matching label.
NodeSelectors vs Taints/Tolerations
NodeSelectors ‚Üí ‚ÄúOnly run me on specific labeled nodes.‚Äù
Taints & Tolerations ‚Üí ‚ÄúDon‚Äôt run anything here unless it tolerates my taint.‚Äù 

Kubernetes Node Labels, Affinity & Tolerations
Node Labels ‚Üí Key/value pairs attached to nodes (e.g., disktype=ssd, env=prod). Used to group and identify nodes.

Add: kubectl label nodes <node-name> key=value
Remove: kubectl label nodes <node-name> key-

Node Affinity ‚Üí Pod-level rules that attract pods to specific nodes based on labels.
Required ‚Üí pod runs only on matching nodes.
Preferred ‚Üí pod tries to run on matching nodes but can fall back.
Taints & Tolerations ‚Üí Node-level rules that repel pods.
Taints block scheduling unless a pod has a matching toleration.

üëâ Difference:

Affinity = pod choosing nodes
Tolerations = nodes restricting pods
----
## REQUESTS AND LIMITS

1. Requests

This is the minimum amount of CPU and memory the container needs to start and run properly.
Kubernetes uses this to decide where to place (schedule) the pod.
üü¢ Think: ‚ÄúGive me at least this much.‚Äù

2. Limits

This is the maximum amount of CPU and memory the container can use.

If the container tries to use more than this:
For CPU: it‚Äôs slowed down (throttled).
For Memory: it can be killed (OOMKilled).
üî¥ Think: ‚ÄúDon‚Äôt let me use more than this.‚Äù

üß© Example

resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "500m"
-----------------------
### HPA & VPA

Scaling Types:
1 Horizontal autoscaler (also known as scale in/out)
    HPA changes the number of pods based on load.
    we do it on 2 different aspect first is workloads(like on pods{HPA}) and second is on infra(on nodes{CLUSTER autoscaler})
    
 >>>  kubectl autoscale deployment myapp --cpu-percent=70 --min=3 --max=10
  ‚úÖ When to use HPA:
  You want to handle varying traffic.
  Your app can run multiple replicas easily (stateless apps).

2 Vertical autoscaler (also known as scale up/down)
   VPA changes the resources (CPU & memory) given to each pod ‚Äî not the number of pods.
  üìà If your app needs more power ‚Üí adds CPU/memory
  üìâ If it needs less ‚Üí reduces CPU/memory
   we do it on 2 different aspect first is workloads(like on pods{HPA}) and second is on infra(on nodes{CLUSTER autoscaler})

  ‚úÖ When to use VPA:
    
    You have a few pods that must stay single (like a database).
    Your app is stateful or not easily scaled horizontally.
    You want to optimize resource usage automatically.
