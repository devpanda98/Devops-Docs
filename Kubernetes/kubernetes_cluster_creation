Kubernetes cluster creation with Kubeadm:

in master node open these ports : 

6443 for kubeapi server
10259 for kube scheduler 
10250 for kubelet
10257 for kube controller manager 
2379 - 2380 if you useing multi cluster node the use this for etcd 

then add for worker node 

10256 for kube proxy 
10250 for kubelet
30000 - 32767 for node port services

-------------------------------
‚öôÔ∏è 1. Prepare the System (All Nodes)
Step 1: Update packages

sudo apt update && sudo apt upgrade -y

Step 2: Disable swap
Kubernetes requires swap to be off.

sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab

Step 3: Load required kernel modules

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

Step 4: Set sysctl parameters for Kubernetes networking :

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system


üêã 2. Install Container Runtime (All Nodes)
You can use containerd, which is preferred.

Step 1: Install dependencies

sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release

Step 2: Install containerd

sudo apt install -y containerd

Step 3: Generate default configuration and enable systemd cgroup driver 

sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml >/dev/null
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
sudo systemctl restart containerd
sudo systemctl enable containerd

Step 4. Add the correct new Kubernetes signing key and repo

sudo mkdir -p /usr/share/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | \
sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-apt-keyring.gpg

Then add the repository:

echo "deb [signed-by=/usr/share/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /" | \
sudo tee /etc/apt/sources.list.d/kubernetes.list


Step 5. Update package list and install

sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

Step 6. Verify installation:

kubeadm version
kubectl version --client
kubelet --version

üñ•Ô∏è . Initialize the Master Node
Step 1: Initialize control plane

(Use your master‚Äôs private IP in --apiserver-advertise-address)

sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=$(hostname -I | awk '{print $1}')

Note: --pod-network-cidr depends on your CNI (Flannel uses 10.244.0.0/16).

Step 2: Configure kubectl for the current user

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Step 3: Verify cluster status

kubectl get nodes
kubectl get pods -A


üåê 5. Install Pod Network Add-on (on Master)
Example using Flannel:
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

Wait for pods to become ready:
kubectl get pods -n kube-system


üë∑ 6. Join Worker Nodes to Cluster

On the master, run:

kubeadm token create --print-join-command

kubectl get nodes

$$  multinode cluster setup $$
----------------------------------

create a vpc 
then create security group for master node , worker node and for load balancer.

in mster node security group :

6443 for kubeapi server
10259 for kube scheduler 
10250 for kubelet
10257 for kube controller manager 
2379 - 2380 if you useing multi cluster node the use this for etcd 


then add for worker node sg :

10256 for kube proxy 
10250 for kubelet
30000 - 32767 for node port services 

CONFIGURE THE LOADBALANCER :
-----------

1Ô∏è‚É£ Install HAProxy
sudo apt update
sudo apt install -y haproxy

2Ô∏è‚É£ Configure HAProxy

Edit the config file:
sudo nano /etc/haproxy/haproxy.cfg

Replace the content (or add at the end) with:

#---------------------------------------------------------------------
# HAProxy Kubernetes API Server Load Balancer
#---------------------------------------------------------------------

frontend k8s-apiserver
    bind *:6443
    mode tcp
    option tcplog
    default_backend k8s-masters

backend k8s-masters
    mode tcp
    balance roundrobin
    option tcp-check
    server master1 10.0.27.194:6443 check fall 3 rise 2
    server master2 10.0.27.105:6443 check fall 3 rise 2
    # Add more masters here if needed


Explanation:

frontend k8s-apiserver ‚Üí Listens on port 6443 (Kubernetes API)
backend k8s-masters ‚Üí Round-robin load balancing between master nodes
tcp-check ‚Üí health checks on the TCP port 6443
master1, master2 ‚Üí Private IPs of your master nodes
Adjust the IPs to your actual master nodes.

3Ô∏è‚É£ Enable and restart HAProxy
sudo systemctl enable haproxy
sudo systemctl restart haproxy
sudo systemctl status haproxy


HAProxy is now listening on port 6443 and forwarding to your masters

4Ô∏è‚É£ Update kubeadm controlPlaneEndpoint

When initializing the first master (or re-initializing):

sudo kubeadm init \
  --control-plane-endpoint "<HAProxy-IP-or-DNS-PRIVATEip>:6443" \
  --upload-certs \
  --pod-network-cidr=10.244.0.0/16


<HAProxy-IP-or-DNS> ‚Üí The private IP or DNS of the Ubuntu server running HAProxy
The rest of the process (certificate key, join tokens) remains the same

5Ô∏è‚É£ Notes / Best Practices

Master nodes firewall/security group: allow 6443 from HAProxy IP and 2379-2380 between masters
HAProxy health checks: tcp-check is enough; can also add SSL checks if using https
Multiple HAProxy instances: for HA, you can have two HAProxy nodes using keepalived for virtual IP

