Kubernetes cluster creation with Kubeadm:

in master node open these ports : 

6443 for kubeapi server
10259 for kube scheduler 
10250 for kubelet
10257 for kube controller manager 
2379 - 2380 if you useing multi cluster node the use this for etcd 

then add for worker node 

10256 for kube proxy 
10250 for kubelet
30000 - 32767 for node port services

-------------------------------
‚öôÔ∏è 1. Prepare the System (All Nodes)
Step 1: Update packages

sudo apt update && sudo apt upgrade -y

Step 2: Disable swap
Kubernetes requires swap to be off.

sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab

Step 3: Load required kernel modules

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

Step 4: Set sysctl parameters for Kubernetes networking :

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system


üêã 2. Install Container Runtime (All Nodes)
You can use containerd, which is preferred.

Step 1: Install dependencies

sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release

Step 2: Install containerd

sudo apt install -y containerd

Step 3: Generate default configuration and enable systemd cgroup driver 

sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml >/dev/null
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
sudo systemctl restart containerd
sudo systemctl enable containerd

Step 4. Add the correct new Kubernetes signing key and repo

sudo mkdir -p /usr/share/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | \
sudo gpg --dearmor -o /usr/share/keyrings/kubernetes-apt-keyring.gpg

Then add the repository:

echo "deb [signed-by=/usr/share/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /" | \
sudo tee /etc/apt/sources.list.d/kubernetes.list


Step 5. Update package list and install

sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

Step 6. Verify installation:

kubeadm version
kubectl version --client
kubelet --version

üñ•Ô∏è . Initialize the Master Node
Step 1: Initialize control plane

(Use your master‚Äôs private IP in --apiserver-advertise-address)

sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=$(hostname -I | awk '{print $1}')

Note: --pod-network-cidr depends on your CNI (Flannel uses 10.244.0.0/16).

Step 2: Configure kubectl for the current user

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Step 3: Verify cluster status

kubectl get nodes
kubectl get pods -A


üåê 5. Install Pod Network Add-on (on Master)
Example using Flannel:
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

Wait for pods to become ready:
kubectl get pods -n kube-system


üë∑ 6. Join Worker Nodes to Cluster

On the master, run:

kubeadm token create --print-join-command

kubectl get nodes

$$  multinode cluster setup $$
----------------------------------

create a vpc 
then create security group for master node , worker node and for load balancer.

in mster node security group :

6443 for kubeapi server
10259 for kube scheduler 
10250 for kubelet
10257 for kube controller manager 
2379 - 2380 if you useing multi cluster node the use this for etcd 


then add for worker node sg :

10256 for kube proxy 
10250 for kubelet
30000 - 32767 for node port services 

CONFIGURE THE LOADBALANCER :
-----------

1Ô∏è‚É£ Install HAProxy
sudo apt update
sudo apt install -y haproxy

2Ô∏è‚É£ Configure HAProxy

Edit the config file:
sudo nano /etc/haproxy/haproxy.cfg

Replace the content (or add at the end) with:

#---------------------------------------------------------------------
# HAProxy Kubernetes API Server Load Balancer
#---------------------------------------------------------------------

frontend k8s-apiserver
    bind *:6443
    mode tcp
    option tcplog
    default_backend k8s-masters

backend k8s-masters
    mode tcp
    balance roundrobin
    option tcp-check
    server master1 10.0.27.194:6443 check fall 3 rise 2
    server master2 10.0.27.105:6443 check fall 3 rise 2
    # Add more masters here if needed


Explanation:

frontend k8s-apiserver ‚Üí Listens on port 6443 (Kubernetes API)
backend k8s-masters ‚Üí Round-robin load balancing between master nodes
tcp-check ‚Üí health checks on the TCP port 6443
master1, master2 ‚Üí Private IPs of your master nodes
Adjust the IPs to your actual master nodes.

3Ô∏è‚É£ Enable and restart HAProxy
sudo systemctl enable haproxy
sudo systemctl restart haproxy
sudo systemctl status haproxy


HAProxy is now listening on port 6443 and forwarding to your masters

4Ô∏è‚É£ Update kubeadm controlPlaneEndpoint

When initializing the first master (or re-initializing):

sudo kubeadm init \
  --control-plane-endpoint "<HAProxy-IP-or-DNS-PRIVATEip>:6443" \
  --upload-certs \
  --pod-network-cidr=10.244.0.0/16


<HAProxy-IP-or-DNS> ‚Üí The private IP or DNS of the Ubuntu server running HAProxy
The rest of the process (certificate key, join tokens) remains the same



‚úÖ How to fix /proc/sys/net/ipv4/ip_forward error
1. Enable IP forwarding temporarily (until reboot)
sudo sysctl -w net.ipv4.ip_forward=1

2. Enable IP forwarding permanently
sudo sed -i 's/#net.ipv4.ip_forward=.*/net.ipv4.ip_forward=1/' /etc/sysctl.conf
sudo sysctl -p

sysctl -w changes it immediately.
Editing /etc/sysctl.conf ensures it persists after reboot.

3. Verify
cat /proc/sys/net/ipv4/ip_forward
Should output: 1
‚úÖ After this
You can re-run your kubeadm join or kubeadm init command, and this particular preflight error will be gone.

5Ô∏è‚É£ Notes / Best Practices

Master nodes firewall/security group: allow 6443 from HAProxy IP and 2379-2380 between masters
HAProxy health checks: tcp-check is enough; can also add SSL checks if using https
Multiple HAProxy instances: for HA, you can have two HAProxy nodes using keepalived for virtual IP

-----------

Accessing a Multi-Master Kubernetes Cluster via HAProxy from an External VM
1Ô∏è‚É£ Cluster Setup

You have a multi-master Kubernetes cluster.
HAProxy is used as a load balancer in front of the master nodes.
HAProxy must be in TCP mode (not HTTP/HTTPS) on port 6443, forwarding traffic to all masters.

2Ô∏è‚É£ Making HAProxy reachable externally

Assign a public IP to HAProxy (or use a hostname).
Update the HAProxy Security Group / firewall rules to allow:

Protocol: TCP
Port: 6443
Source: External VM IP (avoid 0.0.0.0/0 in production).

3Ô∏è‚É£ Preparing kubeconfig

Copy the kubeconfig (/etc/kubernetes/admin.conf) from a master node to the external VM:
scp user@master-node:/etc/kubernetes/admin.conf ~/admin.conf

Or manually copy the YAML content.
Update the server: field under clusters to point to HAProxy public IP:

server: https://<HAProxy_PUBLIC_IP>:6443

Place kubeconfig in default location:

mkdir -p ~/.kube
cp ~/admin.conf ~/.kube/config

4Ô∏è‚É£ Install kubectl on Ubuntu

Direct binary method (recommended):

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" && \
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl && \
kubectl version --client

5Ô∏è‚É£ Verify connectivity

Test HAProxy API endpoint:
curl -vk https://<HAProxy_PUBLIC_IP>:6443/healthz


Should return ok.

If ‚ÄúHTTP request to HTTPS server‚Äù appears, use https://.
Test kubectl (TLS may fail if cert is legacy):
kubectl --insecure-skip-tls-verify=true get nodes --kubeconfig=~/.kube/config

6Ô∏è‚É£ Troubleshooting common issues
Issue	Cause	Fix
TLS: certificate relies on legacy CN	kube-apiserver cert lacks SANs	Use --insecure-skip-tls-verify or regenerate cert with SANs
‚Äúserver rejected our request‚Äù	HAProxy misconfigured or kubeconfig invalid	Ensure HAProxy TCP mode, full client cert/key in kubeconfig
Cannot reach HAProxy	Firewall/SG rules	Open port 6443 for external VM
7Ô∏è‚É£ Permanent setup for convenience

Copy kubeconfig to ~/.kube/config

Set alias if skipping TLS temporarily:
alias kubectl='kubectl --insecure-skip-tls-verify=true'
Add to ~/.bashrc for persistence.

8Ô∏è‚É£ Securing TLS properly (recommended)

Regenerate kube-apiserver certs with HAProxy SAN:
sudo kubeadm init phase certs apiserver --apiserver-cert-extra-sans=<HAProxy_PUBLIC_IP>


Regenerate admin kubeconfig:
sudo kubeadm init phase kubeconfig admin

Restart kube-apiserver (if not static pod)

Copy updated kubeconfig to external VM

Remove --insecure-skip-tls-verify from alias/commands

9Ô∏è‚É£ Final test
kubectl get nodes
kubectl get pods --all-namespaces


Should work from external VM securely via HAProxy.
