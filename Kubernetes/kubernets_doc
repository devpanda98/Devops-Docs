Kubernetes Notes
1. Pods in Kubernetes

Pod: The smallest deployable unit in Kubernetes.
Defined using YAML manifest files.
You can check supported API versions with:
kubectl explain pod

2. YAML Syntax (Key Concepts)

a) spec.selector.matchLabels
Defines which Pods a Deployment should manage.
Works like a filter: ‚ÄúSelect Pods with these labels.‚Äù
b) spec.template.metadata.labels
Labels that are actually applied to Pods when created.
Defines what labels each Pod will have.

3. Deployments & Replica Management

a) ReplicationController
Ensures a specified number of Pods are always running.
Uses:
Selector ‚Üí to match and manage Pods by labels.
template.metadata.labels ‚Üí applies labels to new Pods.

b) Deployment
Manages ReplicaSets (which in turn manage Pods).
Provides features like updates, rollbacks, and scaling.

4. Updating a Deployment

a) Rolling Updates (Default Strategy)
Ensures zero downtime updates.
Process:
Kubernetes creates new Pods with the updated spec (e.g., new image).
Waits until new Pods are Ready.
Gradually deletes old Pods while keeping the desired number available.

b) ReplicaSet During Updates
When Deployment is updated:
A new ReplicaSet is created with the updated Pod spec.
Kubernetes slowly scales up the new ReplicaSet while scaling down the old one.
For each new Pod that becomes Ready, one old Pod is deleted.

Pod in K8s

kubectl explain pod ‚Üí use this command to get the API version that your K8s supports.

YAML Syntax
spec.selector.matchLabels:

Tells the Deployment which Pods it should manage.
Like saying: ‚ÄúLook for Pods with these labels and treat them as part of this Deployment.‚Äù

spec.template.metadata.labels:
Labels applied to the Pods when they are created.
Defines what labels each Pod will have.

Deployments / Replication Controller / Replicas
-------------------------------------------------------

ReplicationController ensures a specified number of Pods are always running.
It uses selector to manage Pods based on their labels.
It uses template.metadata.labels to apply labels to new Pods.

Updating a Deployment

When you change the Deployment (for example, updating the container image), Kubernetes handles it smoothly using Rolling Updates.
Rolling Update Process:
Kubernetes creates new Pods with the updated spec.
It waits for the new Pods to become ready.
It gradually deletes the old Pods while keeping the desired number of Pods available.
This continues until all old Pods are gone.
‚û°Ô∏è This process is called a Rolling Update.

Summary of Rollout Commands
----------------------
Command	Purpose
kubectl apply -f deployment.yaml	Apply or update Deployment
kubectl rollout status deployment my-app	Check rollout progress
kubectl rollout history deployment my-app	View rollout history
kubectl rollout undo deployment my-app	Rollback to previous version
kubectl rollout undo deployment my-app --to-revision=2	Rollback to a specific version
kubectl rollout pause deployment my-app	Pause rollout process
kubectl rollout resume deployment my-app	Resume rollout process
kubectl describe deployment my-app	View rollout details
kubectl rollout restart deployment my-app	Restart the Pods under deployment

Service Types
=------------------

ClusterIP
NodePort
LoadBalancer
ExternalName

# Example ExternalName service:

spec:
  selector:
    app: nginx-app
  type: ExternalName
  externalName: myapp.com
  ports:
  - port: 80
    targetPort: 80 

# Namespace

If a Pod is in a Namespace and another Pod is in a different Namespace, to connect them you must use the other Pod‚Äôs FQDN.

FQDN syntax:

**  service_name.namespace.svc.cluster.local

Example:
demo-service.demo_space.svc.cluster.local

##Sidecar vs Init Containers
---------------------------------
*Init Containers

If you add an init container in the Deployment, first the init container will start, then the main container will start.
Example:

initContainers:
- image: busybox:1.35
  imagePullPolicy: Never
  command: ['sh', '-c']
  args: ['echo Init setup']


‚úî Commands in args are executed inside the init container‚Äôs environment.
‚úî To share data with the main container, use shared volumes like emptyDir.
‚úî Once the init container finishes, the main container starts with whatever setup the init container performed.

Sidecar Containers
-----------------------------------------------
Run alongside the main container in the same Pod.

Share:
Network (same localhost).
Storage volumes (share data).

###Typical use cases:
Logging/monitoring agents.
Proxy containers (e.g., Envoy sidecar in Istio).
Updating or syncing configuration files.

DaemonSets
--------------------
Runs one Pod per Node.
Automatic behavior:
When new nodes are added, DaemonSet automatically adds the Pod.

###Use cases:
Monitoring agents (e.g., Prometheus Node Exporter).
Logging agents (e.g., Fluentd, Filebeat).
Network plugins (e.g., CNI plugins).
Security or backup tools.

CronJobs
Example:
* * * * * echo "Hello World" >> /home/yourusername/hello.log

Cron Syntax (5 fields):
Field	Meaning	Values
Minute	0‚Äì59	*
Hour	0‚Äì23	*
Day of Month	1‚Äì31	*
Month	1‚Äì12	*
Day of Week	0‚Äì6 (Sun=0)	*
Examples:

Run at 10 PM every Tuesday:
0 22 * * 2 echo "Hello World" >> /home/yourusername/hello.log
Run every 5 minutes:
*/5 * * * * echo "Hello World" >> /home/yourusername/hello.log

Where to use a CronJob:
--------------------------
‚úÖ Scheduled recurring tasks:
Backing up data every night at 2 AM.
Cleaning temporary files weekly.
Syncing data between services regularly.
‚úÖ Tasks that repeat and don‚Äôt need manual intervention.

Jobs
------------------
Where to use a Job:

‚úÖ One-time tasks:
Database migration before deployment.
Processing a large batch of files once.
Sending a notification after an event.
‚úÖ Tasks that must complete and where you need to track success/failure.  

** Static Pods
------------------
A Static Pod is managed directly by the kubelet on a node (not by the API server).

***  Commonly used for control plane components:

/etc/kubernetes/manifests/kube-apiserver.yaml
/etc/kubernetes/manifests/kube-scheduler.yaml
/etc/kubernetes/manifests/kube-controller-manager.yaml
/etc/kubernetes/manifests/etcd.yaml

If you edit one of these files, kubelet automatically applies changes and restarts the Pod.

‚úÖ Summary:

‚úî Scheduler, API server, controller manager, etcd ‚Üí run as static pods.
‚úî Managed by kubelet, not the API server.
‚úî Always kept running on control plane nodes.
‚úî Critical for cluster functionality.

** Manual Scheduling
You explicitly tell Kubernetes where to run a Pod.
Options:
nodeName ‚Üí schedule Pod to a specific node.
nodeSelector / affinity ‚Üí more flexible placement rules.

** Useful when:
Strict control of Pod placement is required.
Specific hardware/resources needed.
Even if the scheduler is down, Pods can still be scheduled (since node placement is already defined).

Labels and Selectors
---------------------------------------
Selector = which Pods to manage.
Pod template labels = labels given to new Pods.
Must match ‚Üí otherwise Deployment won‚Äôt recognize its Pods.

Taints & Tolerations
--------------------------------------------
1Ô∏è‚É£ Taints (on Nodes)

Prevent Pods from being scheduled on a node unless tolerated.
** Syntax:
key=value:effect

####Effects of taints:

NoSchedule ‚Üí Pods without a matching toleration will not be scheduled on this node.
PreferNoSchedule ‚Üí Scheduler will try to avoid placing pods on this node, but it‚Äôs not guaranteed.
NoExecute ‚Üí Pods without a matching toleration will be evicted if already running, and new ones won‚Äôt be scheduled.

##   kubectl taint node node1 key=value:NoSchedule

‚û°Ô∏è Node node1 will not accept Pods unless they tolerate this taint.
2Ô∏è‚É£ Tolerations (on Pods)
Allow a Pod to run on nodes with matching taints.
Example:

tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"

‚û°Ô∏è Pod can now be scheduled on nodes tainted with key=value:NoSchedule.

### NodeSelectors
-----------
You can add labels to nodes:
kubectl label node node_name key=value
Then in the Pod spec:

nodeSelector: 
  key: "value"

‚û°Ô∏è Pod will only be scheduled on nodes with the matching label.
NodeSelectors vs Taints/Tolerations
NodeSelectors ‚Üí ‚ÄúOnly run me on specific labeled nodes.‚Äù
Taints & Tolerations ‚Üí ‚ÄúDon‚Äôt run anything here unless it tolerates my taint.‚Äù 

Kubernetes Node Labels, Affinity & Tolerations
Node Labels ‚Üí Key/value pairs attached to nodes (e.g., disktype=ssd, env=prod). Used to group and identify nodes.

Add: kubectl label nodes <node-name> key=value
Remove: kubectl label nodes <node-name> key-

Node Affinity ‚Üí Pod-level rules that attract pods to specific nodes based on labels.
Required ‚Üí pod runs only on matching nodes.
Preferred ‚Üí pod tries to run on matching nodes but can fall back.
Taints & Tolerations ‚Üí Node-level rules that repel pods.
Taints block scheduling unless a pod has a matching toleration.

üëâ Difference:

Affinity = pod choosing nodes
Tolerations = nodes restricting pods
----
## REQUESTS AND LIMITS

1. Requests

This is the minimum amount of CPU and memory the container needs to start and run properly.
Kubernetes uses this to decide where to place (schedule) the pod.
üü¢ Think: ‚ÄúGive me at least this much.‚Äù

2. Limits

This is the maximum amount of CPU and memory the container can use.

If the container tries to use more than this:
For CPU: it‚Äôs slowed down (throttled).
For Memory: it can be killed (OOMKilled).
üî¥ Think: ‚ÄúDon‚Äôt let me use more than this.‚Äù

üß© Example

resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "500m"
-----------------------
### HPA & VPA

Scaling Types:
1 Horizontal autoscaler (also known as scale in/out)
    HPA changes the number of pods based on load.
    we do it on 2 different aspect first is workloads(like on pods{HPA}) and second is on infra(on nodes{CLUSTER autoscaler})
    
 >>>  kubectl autoscale deployment myapp --cpu-percent=70 --min=3 --max=10
  ‚úÖ When to use HPA:
  You want to handle varying traffic.
  Your app can run multiple replicas easily (stateless apps).

2 Vertical autoscaler (also known as scale up/down)
   VPA changes the resources (CPU & memory) given to each pod ‚Äî not the number of pods.
  üìà If your app needs more power ‚Üí adds CPU/memory
  üìâ If it needs less ‚Üí reduces CPU/memory
   we do it on 2 different aspect first is workloads(like on pods{HPA}) and second is on infra(on nodes{CLUSTER autoscaler})

  ‚úÖ When to use VPA:
    
    You have a few pods that must stay single (like a database).
    Your app is stateful or not easily scaled horizontally.
    You want to optimize resource usage automatically.

‚úÖ Health Probes ###
----------------------


‚úÖ configmaps and secrets :
---------
configmap: 
 you can use the configmaps as environment variable and as volume mounts
 to create and add as a volume mount create the configmap and in deployment yaml  add volumeMounts and volume
 ex:  
  spec: 
    containers:
    -  name: dummy-app
       image: python3:1.0
       volumeMounts: 
       - name: my-db-vol   ## name is same as the volume
         mountPath: /opt
    volumes:
    -  name: my-db-vol    ## volume name 
       ConfigMap:
          name: test-cm   ## Configmap name

$$$ Secrets:
------------
imperative way :
   kubectl create secret generic db-secret \
  --from-literal=username=admin \
  --from-literal=password=MyP@ssw0rd
Declarative way :
----------
      apiVersion: v1
      kind: Secret
      metadata:
        name: db-secret
      type: Opaque
      data:
        username: YWRtaW4=        # base64 for "admin"
        password: TXlQQHNzdzByZA==  # base64 for "MyP@ssw0rd"


how to use secret in deployment yaml:
 -----------
  spec:
  containers:
    - name: demo-container
      image: nginx
      env:
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
  

üîíCERTIFICATES IN KUBERNETES üîí
-------------------------------

üîë how ssl/Tls works: 
----
* symmetric encyption , asymmetric encryption

When a client sends a GET request over HTTPS:
The server has a public and private key pair.
The server sends its public key (inside a certificate) to the client.
The client verifies the certificate and then generates a random pre-master secret.
The client encrypts this secret using the server‚Äôs public key and sends it.
The server decrypts it with its private key.
Both sides now use that secret to create a shared session key for encrypted communication.

### Manage TLS Certificates In a Kubernetes Cluster - Create Certificate Signing Request:
----------
public certificate : anything that has .crt & .pem as an extention is a public certificate . ex :  server.crt,server.pem,client.crt,client.pem
Private Key : anything that has key in the name or extention is private key . ex: server.key, server-key.pem 

Create user certificates :
  1.  openssl genrsa -out user.key 2048 (this command will generate the private key for user )
  2. openssl req -new -key user.key -out user.csr -subj "/CN=username" (User generates a Certificate Signing Request (CSR))

after these 2 steps you will get user.csr and user.key files 

  3. User sends the CSR to the Admin :
The user sends the CSR to the administrator who will be responsible for approving the request and signing the certificate.
4. create a csr.yaml so that when you apply that yaml it will create a scr request object .


apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: user-csr
spec:
  request: <base64-encoded-csr>
  signerName: kubernetes.io/kubelet-apiserver-client
  usages:
  - digital signature
  - key encipherment
  - client auth

here in the request section you have to paste the csr data which is base64 encoded ,to convert the data to base64 use this command , whcih will turn 
the csr to base64 format and print that in one line , paste that in that request section in the yaml .

>>>  cat user.csr | base64 | tr -d "\n"

after this you apply the yaml (kubectl apply user-csr.yaml) can check by : kubectl get csr 
The request field should be set to the base64-encoded CSR generated by the user. The signerName field should be set to the name of the certificate signer that will be used to sign the certificate. The usages field should be set to the list of intended usages for the certificate, which in this case includes digital signature, key encipherment, and client authentication.
it will give the csr object and it might show as pending , cause its not approved by any CA authority yet ,

5. Admin approves or rejects the CSR :
  get list of csr :  kubectl get csr
  approve csr :  kubectl certificate approve user-csr
  deny the csr :   kubectl certificate deny user-csr


The kube-controller-manager detects the approval.
It signs the CSR using the cluster‚Äôs CA key and certificate.
The signed certificate is stored back in the CSR‚Äôs status.certificate field.
You can extract the signed certificate as follows:

>>>>>> kubectl get csr <csr-name> -o jsonpath='{.status.certificate}' | base64 -d > client.crt

6............Cluster CA Configuration
The Kubernetes cluster CA is managed by the kube-controller-manager.
The CA‚Äôs key and certificate are usually located on the control plane node(s):
              /etc/kubernetes/pki/ca.crt
              /etc/kubernetes/pki/ca.key
The controller manager uses these flags to reference them:
          --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
          --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
If these files or flags are missing, approved CSRs will not result in signed certificates

‚úÖ Kubernetes Authentication and Authorization :
------
SO when we use any kubectl command to interact with the server somehow it knows what permissions we have and based on that it is showing the results .

Types of authorization:
-----
| Type        | Description                       | Common Use                        | Managed By        |
| ----------- | --------------------------------- | --------------------------------- | ----------------- |
| **RBAC**    | Role-based access control         | Default choice for most clusters  | Cluster Admin     |
| **ABAC**    | Attribute-based (JSON policies)   | Legacy/testing                    | Cluster Admin     |
| **Node**    | Limits node (Kubelet) permissions | Internal system use               | Kubernetes itself |
| **Webhook** | Delegates to external service     | Custom or enterprise auth systems | External service  |


‚úÖ Role Based Access Control Kubernetes: 
------
kubernetes does not deal with user management , it offloads it to identity providers like : aws IAM, LDAP, Azure active Dairectory
RBAC (Role-Based Access Control) controls who can do what within a Kubernetes cluster.

It uses four key objects: Role, ClusterRole, RoleBinding ,ClusterRoleBinding
üî∏ 1. Role : A Role defines a set of permissions (like read, write, delete) for resources within a specific namespace.
üî∏ 2. ClusterRole: A ClusterRole is similar to a Role ‚Äî but it‚Äôs cluster-wide.
    It can:
    Grant permissions across all namespaces, or
    Be used for non-namespaced resources (like nodes, persistent volumes, etc.)

RoleBinding ‚Üí connects a Role to a user or service account in a namespace
ClusterRoleBinding ‚Üí connects a ClusterRole to a user or service account cluster-wide

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: development
rules: 
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
##  here in role yaml you have to mentiones rules: apigroups , resources(that you want to add for the role), verbs (here you add what kind of access you want )

### Rolebinding:
apiVersion: rbac.authorization.k8s.io/v1
Kind: RoleBinding
metadata: 
   name: read-pods-binding
   namespace: development
subjects:
- Kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io 
RoleRef:
- kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

## in RoleBinding you have to add subjects: under that add - kind:User, name of user, apigroup that is you added in the apiVersion
next add RoleRef , this is the role you want to bind with the user, in this add Roleref: kind: Role, name of the role you have created,
apiGroup
### Cluster Role and Role binding: this is for the entire cluster , you shouldnot mention namespace in the defining yaml .

üì¶ Core Group and Not Core group :
1Ô∏è‚É£ Core group :
The oldest, built-in resources in Kubernetes.
They don‚Äôt have a group name ‚Äî they‚Äôre just part of the main API.
When writing RBAC, you show this with apiGroups: [""] (empty string).
üì¶ Examples of core group resources:  Pods,Services,ConfigMaps,Secrets,Namespaces,Nodes,PersistentVolumeClaims

üîπ 2Ô∏è‚É£ Non-core (named) groups:
Newer or specialized features are put in separate groups.
Each one has a group name like apps, batch, networking.k8s.io, etc.
In RBAC, you write that name.

| Resource               | API group                 | How to write in RBAC                       |
| ---------------------- | ------------------------- | ------------------------------------------ |
| Deployments            | apps                      | `apiGroups: ["apps"]`                      |
| Jobs, CronJobs         | batch                     | `apiGroups: ["batch"]`                     |
| Roles, ClusterRoles    | rbac.authorization.k8s.io | `apiGroups: ["rbac.authorization.k8s.io"]` |
| Ingress, NetworkPolicy | networking.k8s.io         | `apiGroups: ["networking.k8s.io"]`         |


####  and User and Groups belongs to the api group 

User and Group objects are not real Kubernetes resources in the API server.
They are identities managed outside Kubernetes (like in kubeconfig or an identity provider).
Kubernetes defines these kinds (User, Group) within the RBAC API group ‚Äî hence rbac.authorization.k8s.io.

| Subject kind       | Example                           | Correct `apiGroup` value    |
| ------------------ | --------------------------------- | --------------------------- |
| **User**           | `name: alice`                     | `rbac.authorization.k8s.io` |
| **Group**          | `name: dev-team`                  | `rbac.authorization.k8s.io` |
| **ServiceAccount** | `name: app-sa`, `namespace: prod` | `""` *(empty string)*       |

| Location                     | Key         | Example | Notes                       |
| ---------------------------- | ----------- | ------- | --------------------------- |
| In `rules:`                  | `apiGroups` | `[""]`  | List ‚Äî for resource groups  |
| In `subjects:` or `roleRef:` | `apiGroup`  | `""`    | String ‚Äî for RBAC API group |

üß© Rule of Thumb
‚û°Ô∏è Roles are namespaced objects, so if you don‚Äôt include it, the Role might not apply where you expect.
   A single Role can have multiple rules: blocks,
   and each block can define access to one or more resources (pods, configmaps, secrets, etc.).
   Then you bind that one Role to a User, Group, or ServiceAccount using a single RoleBinding.
‚û°Ô∏è Each Role needs its own RoleBinding,
even if you‚Äôre binding them to the same ServiceAccount, User, or Group.
üß† Why?
Because:
A RoleBinding connects one Role to one or more subjects.
It doesn‚Äôt support binding multiple Roles at once.
So if your ServiceAccount needs permissions from two different Roles,
‚Üí you just create two RoleBindings, both pointing to the same ServiceAccount

In RBAC, main resources and subresources are treated separately.
So if you want to do something like kubectl logs or kubectl exec,
you must give permissions for the subresource ‚Äî not just the main resource
‚úÖ service accounts:
--------------------
A ServiceAccount is an identity for pods.
Pods use it when they need to talk to the Kubernetes API (like listing pods, reading ConfigMaps, etc.).
By default, every pod uses the default ServiceAccount, which has no special permissions.
You create your own ServiceAccount when your app needs specific access.
You then bind Roles or ClusterRoles to it using RoleBinding or ClusterRoleBinding.
This gives the pod only the permissions you choose ‚Äî nothing more.

apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-serviceaccount
  namespace: default

>>>>>  to attach a Pod to SA you need to add ""ServiceAccountName"" in Pod specs in deployment yaml

üß† 3Ô∏è‚É£ Why use your own ServiceAccount anyway?

Even though pods ‚Äúwork‚Äù with the default SA, using a custom one is best practice because:
You can give only the exact permissions that pod needs.
You avoid accidentally giving too much power to all pods using the default SA.
It‚Äôs safer and easier to audit later.

üß† SubResource in RBAC :
---------------------------------------
In RBAC, main resources and subresources are treated separately.
So if you want to do something like kubectl logs or kubectl exec,
you must give permissions for the subresource ‚Äî not just the main resource.
üëâ If your kubectl command does an action on a resource instead of editing it directly,
it‚Äôs probably a subresource.

If your command involves logs, exec, attach, scale, status, or approval ‚Üí
it‚Äôs a subresource, so add it as "pods/log" or "deployments/scale" in your RBAC resources: list.

‚ö° In short
Subresources = ‚Äúspecial API actions‚Äù related to a resource.
RBAC needs explicit permission for them.
Trick: if your kubectl command does something to the resource (not just gets it),
it‚Äôs probably a subresource.

‚úÖKubernetes Network Policies :
----------------

üß©DNS ():
------------------
DNS lookup : A DNS lookup is the process of translating a domain name (like www.google.com) into its IP address so computers can connect. 
Your computer checks its cache first, then asks DNS servers in order (root ‚Üí TLD ‚Üí authoritative) until it finds the IP address.

Here‚Äôs how it works step-by-step:
-- You enter a domain name ‚Äî For example, you type www.example.com into your browser.
-- The computer checks local cache ‚Äî It first checks if it already knows the IP address from a previous visit.
-- If not found, it queries a DNS resolver ‚Äî This is usually provided by your Internet Service Provider (ISP) or a public DNS (like Google DNS 8.8.8.8).
-- The resolver contacts DNS servers in a hierarchy:
-- Root DNS servers (tell where to find .com servers)
-- TLD (Top-Level Domain) servers (tell where to find example.com servers)
-- Authoritative DNS servers (store the actual IP address for www.example.com)
-- The resolver returns the IP address to your computer.
-- Your browser connects to the web server using that IP address.

DNS record types üëá: 

| **Record Type** | **Full Form / Meaning**   | **Purpose / Function**                                     | **Example**                                       |
| --------------- | ------------------------- | ---------------------------------------------------------- | ------------------------------------------------- |
| **A**           | Address Record            | Maps a domain to an **IPv4 address**                       | `example.com ‚Üí 192.168.1.1`                       |
| **AAAA**        | IPv6 Address Record       | Maps a domain to an **IPv6 address**                       | `example.com ‚Üí 2001:db8::1`                       |
| **CNAME**       | Canonical Name Record     | Creates an alias to another domain                         | `www.example.com ‚Üí example.com`                   |
| **MX**          | Mail Exchange Record      | Directs email to a mail server                             | `example.com ‚Üí mail.example.com`                  |
| **NS**          | Name Server Record        | Specifies the authoritative DNS servers for the domain     | `example.com ‚Üí ns1.host.com`                      |
| **TXT**         | Text Record               | Holds text info (like SPF, DKIM, domain verification)      | `v=spf1 include:_spf.google.com ~all`             |
| **PTR**         | Pointer Record            | Used for **reverse DNS lookup** (IP ‚Üí domain)              | `192.168.1.1 ‚Üí example.com`                       |
| **SRV**         | Service Record            | Defines the location of specific services (e.g. SIP, LDAP) | `_sip._tcp.example.com ‚Üí sipserver.example.com`   |
| **SOA**         | Start of Authority Record | Contains admin info and DNS zone settings                  | Primary server, contact email, refresh time, etc. |


üß© Core DNS :
------------------
CoreDNS lets Pods find and communicate with each other using service names instead of IPs.

üè∑Ô∏è How CoreDNS Works in Kubernetes (Simple Version): 
üè∑Ô∏è 1. Service gets a DNS name
When you create a Service in Kubernetes, it automatically gets a DNS name like:
my-service.my-namespace.svc.cluster.local
This name is managed by CoreDNS and points to the ClusterIP of the Service.

üì© 2. Pod makes a DNS request
When a Pod tries to connect to a service (like my-service.my-namespace.svc.cluster.local),
it looks at its /etc/resolv.conf file.
inside th resolv.conf file you will find 
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5

the nameserver ip is the Ip of CoreDNS svc , so the request from the pod goes to this ip 
That file tells the Pod to send DNS queries to the CoreDNS service IP (for example, 10.96.0.10).

üåê 3. DNS query goes to CoreDNS
The DNS request is sent to CoreDNS, which runs as a Deployment in the kube-system namespace.
CoreDNS is exposed through a Service called kube-dns or coredns.
‚öôÔ∏è 4. CoreDNS checks its Corefile
CoreDNS uses a configuration file called the Corefile.
This file has rules and plugins that tell CoreDNS how to process DNS queries.
Example snippet:
.:53 {
    kubernetes cluster.local in-addr.arpa ip6.arpa
    forward . /etc/resolv.conf
    cache 30
}
üîç 5. CoreDNS uses the ‚Äúkubernetes‚Äù plugin
The kubernetes plugin helps CoreDNS look up Service and Pod names inside the cluster.
It talks to the Kubernetes API server to find the correct IP address for the requested service or pod.

üì§ 6. CoreDNS sends a DNS response
CoreDNS replies to the Pod with the ClusterIP of the requested service.
The Pod then connects to that IP, and traffic is load-balanced across the backend Pods of that Service.

üåè 7. Handling external queries
If the domain isn‚Äôt part of the cluster (like google.com),
CoreDNS forwards the query to an external DNS server (defined in its /etc/resolv.conf).

‚úÖ In short:
Pod ‚Üí CoreDNS ‚Üí Kubernetes API ‚Üí CoreDNS reply ‚Üí Pod connects to Service
------------------------------------------------------------------------------------------------------------------
üìù If you feel like you core DNS is not coming up then you should check the networking addOn, like calico 
you should check this should be there first , otherwise the CoreDns pods will not come up.

üìù Notes for Calico Setup on AWS (Kubeadm Cluster)

üü¢ 1. Disable Source/Destination Check
Go to each EC2 instance ‚Üí Actions ‚Üí Networking ‚Üí Change Source/Destination Check ‚Üí Disable
This allows the instance to route traffic for other nodes, which is required because Calico makes each node act like a router.
If not disabled, Pod-to-Pod communication between nodes will fail.

üü£ 2. IP Auto-Detection in Calico
Calico needs to know which IP (network interface) to use on each node.
Enable or configure IP auto-detection so Calico picks the correct internal VPC IP (not docker0 or public IP).
Add this in the Calico DaemonSet (calico-node):

- name: IP_AUTODETECTION_METHOD
  value: "can-reach=8.8.8.8"
This ensures correct Pod routing and BGP communication between nodes.
üß† In Short:
Disable Source/Destination Check ‚Üí allows EC2 nodes to route Pod traffic.
Enable IP Auto-Detection ‚Üí ensures Calico uses the correct internal IP for networking.
‚úÖ Both steps are mandatory for Calico to work properly on AWS EC2 Kubernetes clusters.
--------------------------------------------------------------------------------------------------------------
‚úÖ   CNI (container network interface) :
-----
It‚Äôs a plugin system that tells Kubernetes how Pods get network connectivity ‚Äî
how they get IPs, talk to each other, and reach the outside world.

Think of CNI as the network driver for your 





üß© Pod Networking:
-----------
>By default pods can communicate with each other , there is no limitation.
>if there are multiple containers in a pod like ex: sidecar container in istio, they can communicate with eachother using pods localhost,
but the port should be different for each container.
>but if the container inside one pod cant connect to the container of another pod with localhost 
>



‚úÖ Custom resource definition (CRD)
----------------------------


